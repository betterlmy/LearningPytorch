{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 自然语言统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zane/miniforge3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys,os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 2261),\n ('i', 1267),\n ('and', 1245),\n ('of', 1155),\n ('a', 816),\n ('to', 695),\n ('was', 552),\n ('in', 541),\n ('that', 443),\n ('my', 440)]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据文本预处理中介绍的time_machine数据集构建Vocab,打印前10个最常用的单词\n",
    "tokens = d2l.tokenize(d2l.read_time_machine())\n",
    "coupus = [token for line in tokens for token in line]\n",
    "vocab = d2l.Vocab(coupus)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "词频最高的词往往都是很无聊 被称为 Stop Word,停用词,可以过滤掉."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 读取长序列数据\n",
    "序列数据本质上是连续的.当序列变得过长无法一次性全部处理时候,我们希望拆分这样的序列方便读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 随机采样\n",
    "在随机采样中,每个样本都是在原始长序列上任意捕获的子序列,在迭代过程中,来自两个相邻的 随机的小批量的字序列不一定在原始序列上相邻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "# 下面代码每次可以从数据中随机生成一个小批量,在这里 参数batch_size制定了每个小批量中子序列样本的数目,参数num_steps是每个字序列中预定义 的时间步数.\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "\n",
    "    # 从随机偏移量开始对序列进行分区\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 打乱每个子序列的起始点顺序\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(position):\n",
    "        return corpus[position:position + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0,batch_size*num_batches,batch_size):\n",
    "        initial_indices_perbatch = initial_indices[i:i+batch_size]\n",
    "        X = [data(j) for j in initial_indices_perbatch]\n",
    "        Y = [data(j + 1) for j in initial_indices_perbatch]\n",
    "        yield torch.Tensor(X), torch.Tensor(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19., 20.]]) tensor([[12., 13., 14., 15., 16.],\n",
      "        [17., 18., 19., 20., 21.]])\n",
      "tensor([[21., 22., 23., 24., 25.],\n",
      "        [26., 27., 28., 29., 30.]]) tensor([[22., 23., 24., 25., 26.],\n",
      "        [27., 28., 29., 30., 31.]])\n",
      "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10.]]) tensor([[ 2.,  3.,  4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X,Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print(X,Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def yield_test():\n",
    "    for i in range(5):\n",
    "        yield i\n",
    "for i in yield_test():\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8bc75a386d56b6551c17e1904f8529f6445721752a10015ef716c860769d0c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}