{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Python is not a great way to writing parallel and asynchronous code.\n",
    "Understanding how asynchronous programming works help us tp develop more efficient programs,by proactively reducing computational requirements and mutual dependencies.This allows us to reduce memory overhead and increase processor utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lmy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Asynchrony via backend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparison of numpy and pytorch with matrix multiplication"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lmy' has no attribute 'Timer'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mlmy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTimer\u001B[49m()\n\u001B[1;32m      2\u001B[0m numpyTimer \u001B[38;5;241m=\u001B[39m lmy\u001B[38;5;241m.\u001B[39mTimer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNumpy Timer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m torchTimer \u001B[38;5;241m=\u001B[39m lmy\u001B[38;5;241m.\u001B[39mTimer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch Timer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'lmy' has no attribute 'Timer'"
     ]
    }
   ],
   "source": [
    "\n",
    "numpyTimer = lmy.Timer('Numpy Timer')\n",
    "torchTimer = lmy.Timer('torch Timer')\n",
    "with numpyTimer:\n",
    "    for i in range(30):\n",
    "        a = np.random.normal(size=(1000, 1000))\n",
    "        b = a @ a\n",
    "\n",
    "devices, _ = lmy.getGPU()\n",
    "device = devices[0]\n",
    "with torchTimer:\n",
    "    for i in range(30):\n",
    "        a = torch.randn((1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "\n",
    "with torchTimer:\n",
    "    for i in range(30):\n",
    "        a = torch.randn((1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    if 'cuda' in device.type:\n",
    "        torch.cuda.synchronize(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}