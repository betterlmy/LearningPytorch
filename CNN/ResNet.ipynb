{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "随着设计的网络越来越深,理解新添加的层如何提升性能越发重要,更重要的是网络设计能力."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数类\n",
    "假设有一个神经网络架构F,包括了lr和其他参数,对于所有的f属于F,存在一些参数集合,包含了权重和偏置,这些参数可以在核实的数据集上进行训练而获得.假设$f*$是我们想要的函数,而且$f*$属于F,那我们可以很容易的训练得到他,但是通常不会这么容易.相反 我们尝试找到一个函数$f*F$这是在F中的最佳选择.\n",
    "例如给定一个具有X特征和y标签的数据集,我们可以尝试通过解决以下问题来找到想要的f\\\n",
    "![](img/2022-04-28-14-18-52.png)\\\n",
    "那么.如何获得更接近真实的$f*$呢,唯一合理的可能性是,设计一个更加强大的网络架构$F'$,但是实际上新设计的网络架构可能比原来更弱\\\n",
    "![](img/2022-04-28-14-24-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有当较为复杂的函数类包含较小的函数类的时候,我们才能确保提高他们的性能.对于深度神经网络.如果我们能将新添加的层训练成恒等映射(indentity function) f(x)=x,新模型和原模型将同样有效.同时 由于新模型可能得出更优的解来拟合训练数据集,因此添加层似乎更容易降低训练误差."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对这一问题,ResNet被提出,核心思想:\n",
    "**每个附加层都应该更容易的包含原始函数作为其元素之一.**\n",
    "因此残差块(residual block)便产生了,这个设计对如何建立深层神经网络产生了深远的影响."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block 残差块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focus on the internal of the network, as the shown of the figur7.6.2,suppose that the original input of net is $x$,但是我们想要学出来的理想的映射为f(x),作为activation function的输入, 在左图的虚线框中的部分需要直接拟合出想要的映射f(x),而右图的虚线框部分则需要拟合残差映射$f(x)-x$,在实际中,残差映射更容易拟合和优化.\n",
    "本节开头我们希望学出理想的映射f(x),我们只需要将右图的虚线框内上方的加权运算(如仿射)的权重和偏置参数设置成0,那么f(x)即为恒等映射,实际中,当理想的映射极其接近恒等映射时,残差映射也易于捕捉恒定映射的细微波动.\n",
    "右图即残差块,在RB(residual block)中,输入可以通过跨层数据线路更快地前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/2022-04-28-14-36-38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet 沿用了VGG完整的3X3卷积层设计.在RB中首先有两个相同输出通道的3X3卷积层.每个卷积层后接一个BN(BatchNormal)层和ReLU.\n",
    "然后我们通过跨层数据通路 跳过这两个卷积运算,将输入直接加在最后的ReLU激活函数前,这样的设计要求两个卷积层的输出与输入形状相同.从而使他们可以相加.\n",
    "如果想改变通道数,就需要引入一个额外的1X1卷积块,\n",
    "RB实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import lmy\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        \"\"\"残差块的初始化\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): 输入的通道数\n",
    "            num_channels (int): 输出的最终通道数\n",
    "            use_1x1conv (bool, optional): 是否要使用1x1的卷积来实现通道数的变换. Defaults to False.\n",
    "            strides (int, optional): 步长. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, num_channels,\n",
    "                               kernel_size=3, stride=strides, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(\n",
    "                in_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.conv1(X)\n",
    "        Y = self.bn1(Y)\n",
    "        Y = F.relu(Y)\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.bn2(Y)\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "\n",
    "        Y += X\n",
    "        return F.relu(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残差块如下图\n",
    "![](img/2022-04-28-14-56-13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出和输入形状一致的情况\n",
    "block1 = Residual(3, 3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "X.shape\n",
    "Y = block1(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出和输入形状不一致的情况\n",
    "block2 = Residual(3, 6, use_1x1conv=True, strides=2)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = block2(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet模型\n",
    "![](img/2022-04-29-16-24-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet的前两层与GoogLeNet相同 :在输出通道数为64,步幅为2的7x7卷积后接上步幅为2的3x3MaxPooling,不同之处在于ResNet每个卷积层后增加了BN层 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d torch.Size([2, 64, 112, 112])\n",
      "BatchNorm2d torch.Size([2, 64, 112, 112])\n",
      "ReLU torch.Size([2, 64, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000020?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000020?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m b1:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000020?line=7'>8</a>\u001b[0m     X \u001b[39m=\u001b[39m layer(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000020?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(layer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=160'>161</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=161'>162</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=162'>163</a>\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=163'>164</a>\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/_jit_internal.py:422\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/_jit_internal.py?line=419'>420</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/_jit_internal.py?line=420'>421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/_jit_internal.py?line=421'>422</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:797\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py?line=794'>795</a>\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py?line=795'>796</a>\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py?line=796'>797</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "    nn.BatchNorm2d(64),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")\n",
    "# X = torch.ones((2,1,224,224))\n",
    "# for layer in b1:\n",
    "#     X = layer(X)\n",
    "#     print(layer.__class__.__name__,f\"{X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNet在后面接了四个Inception块,而ResNet字啊后面使用了四个残差块组成的模块ResidualNet,每个模块使用若干个同样输出通道数的残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(in_channels,num_channels,num_residuals,first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(in_channels,num_channels,use_1x1conv=True,strides=2))\n",
    "        else: \n",
    "            blk.append(Residual(num_channels,num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64,64,2,first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64,128,2))\n",
    "b4 = nn.Sequential(*resnet_block(128,256,2))\n",
    "b5 = nn.Sequential(*resnet_block(256,512,2))\n",
    "resNet = nn.Sequential(b1,b2,b3,b4,b5,\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(),nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape: torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape: torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape: torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape: torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape: torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape: torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape: torch.Size([1, 512])\n",
      "Linear output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((1,1,224,224))\n",
    "for layer in resNet:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNIST数据集加载成功，训练集大小:60000,测试集大小:10000,数据集shape:torch.Size([1, 28, 28])\n",
      "设备数量不足,已自动调整\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000035?line=0'>1</a>\u001b[0m lr , num_epochs ,batch_size \u001b[39m=\u001b[39m \u001b[39m.05\u001b[39m,\u001b[39m10\u001b[39m,\u001b[39m256\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000035?line=1'>2</a>\u001b[0m train_iter,test_iter \u001b[39m=\u001b[39m lmy\u001b[39m.\u001b[39mloadFashionMnistData(batch_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000035?line=2'>3</a>\u001b[0m lmy\u001b[39m.\u001b[39;49mtrain_GPU(resNet,train_iter,test_iter,num_epochs,lr)\n",
      "File \u001b[0;32m~/Documents/GitHub/LearningPytorch/lmy/__init__.py:449\u001b[0m, in \u001b[0;36mtrain_GPU\u001b[0;34m(net, train_iter, test_iter, num_epochs, lr, timer, devices, num_devices, init_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/zane/Documents/GitHub/LearningPytorch/lmy/__init__.py?line=445'>446</a>\u001b[0m net\u001b[39m.\u001b[39mapply(init_weight)\n\u001b[1;32m    <a href='file:///Users/zane/Documents/GitHub/LearningPytorch/lmy/__init__.py?line=447'>448</a>\u001b[0m devices \u001b[39m=\u001b[39m devices[:num_devices]\n\u001b[0;32m--> <a href='file:///Users/zane/Documents/GitHub/LearningPytorch/lmy/__init__.py?line=448'>449</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m devices[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mtype:\n\u001b[1;32m    <a href='file:///Users/zane/Documents/GitHub/LearningPytorch/lmy/__init__.py?line=449'>450</a>\u001b[0m     net\u001b[39m.\u001b[39mto(devices[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/zane/Documents/GitHub/LearningPytorch/lmy/__init__.py?line=450'>451</a>\u001b[0m timer\u001b[39m.\u001b[39mstart()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lr , num_epochs ,batch_size = .05,10,256\n",
    "train_iter,test_iter = lmy.loadFashionMnistData(batch_size)\n",
    "lmy.train_GPU(resNet,train_iter,test_iter,num_epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8bc75a386d56b6551c17e1904f8529f6445721752a10015ef716c860769d0c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
