{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "随着设计的网络越来越深,理解新添加的层如何提升性能越发重要,更重要的是网络设计能力."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数类\n",
    "假设有一个神经网络架构F,包括了lr和其他参数,对于所有的f属于F,存在一些参数集合,包含了权重和偏置,这些参数可以在核实的数据集上进行训练而获得.假设$f*$是我们想要的函数,而且$f*$属于F,那我们可以很容易的训练得到他,但是通常不会这么容易.相反 我们尝试找到一个函数$f*F$这是在F中的最佳选择.\n",
    "例如给定一个具有X特征和y标签的数据集,我们可以尝试通过解决以下问题来找到想要的f\\\n",
    "![](img/2022-04-28-14-18-52.png)\\\n",
    "那么.如何获得更接近真实的$f*$呢,唯一合理的可能性是,设计一个更加强大的网络架构$F'$,但是实际上新设计的网络架构可能比原来更弱\\\n",
    "![](img/2022-04-28-14-24-54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有当较为复杂的函数类包含较小的函数类的时候,我们才能确保提高他们的性能.对于深度神经网络.如果我们能将新添加的层训练成恒等映射(indentity function) f(x)=x,新模型和原模型将同样有效.同时 由于新模型可能得出更优的解来拟合训练数据集,因此添加层似乎更容易降低训练误差."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对这一问题,ResNet被提出,核心思想:\n",
    "**每个附加层都应该更容易的包含原始函数作为其元素之一.**\n",
    "因此残差块(residual block)便产生了,这个设计对如何建立深层神经网络产生了深远的影响."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block 残差块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focus on the internal of the network, as the shown of the figur7.6.2,suppose that the original input of net is $x$,但是我们想要学出来的理想的映射为f(x),作为activation function的输入, 在左图的虚线框中的部分需要直接拟合出想要的映射f(x),而右图的虚线框部分则需要拟合残差映射$f(x)-x$,在实际中,残差映射更容易拟合和优化.\n",
    "本节开头我们希望学出理想的映射f(x),我们只需要将右图的虚线框内上方的加权运算(如仿射)的权重和偏置参数设置成0,那么f(x)即为恒等映射,实际中,当理想的映射极其接近恒等映射时,残差映射也易于捕捉恒定映射的细微波动.\n",
    "右图即残差块,在RB(residual block)中,输入可以通过跨层数据线路更快地前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/2022-04-28-14-36-38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet 沿用了VGG完整的3X3卷积层设计.在RB中首先有两个相同输出通道的3X3卷积层.每个卷积层后接一个BN(BatchNormal)层和ReLU.\n",
    "然后我们通过跨层数据通路 跳过这两个卷积运算,将输入直接加在最后的ReLU激活函数前,这样的设计要求两个卷积层的输出与输入形状相同.从而使他们可以相加.\n",
    "如果想改变通道数,就需要引入一个额外的1X1卷积块,\n",
    "RB实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import lmy\n",
    "from icecream import icecream as ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,num_channels,use_1x1conv=False,strides=1): \n",
    "        \"\"\"残差块的初始化\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): 输入的通道数\n",
    "            num_channels (int): 输出的最终通道数\n",
    "            use_1x1conv (bool, optional): 是否要使用1x1的卷积来实现通道数的变换. Defaults to False.\n",
    "            strides (int, optional): 步长. Defaults to 1.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels,num_channels,kernel_size=3,stride=strides,padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_channels,num_channels,kernel_size=3,stride=strides,padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels,num_channels,kernel_size=1,stride=strides)\n",
    "            \n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "    def forward(self,X):\n",
    "        Y = self.conv1(X)\n",
    "        Y = self.bn1(Y)\n",
    "        Y = F.relu(Y)\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.bn2(Y)\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        ic(X.shape)\n",
    "        ic(Y.shape)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残差块如下图\n",
    "![](img/2022-04-28-14-56-13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000014?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m6\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000014?line=3'>4</a>\u001b[0m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000014?line=4'>5</a>\u001b[0m Y \u001b[39m=\u001b[39m block1(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000014?line=5'>6</a>\u001b[0m Y\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 10'\u001b[0m in \u001b[0;36mResidual.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=29'>30</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=30'>31</a>\u001b[0m ic(X\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=31'>32</a>\u001b[0m ic(Y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=32'>33</a>\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# 输出和输入形状一致的情况\n",
    "block1 = Residual(3, 3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "X.shape\n",
    "Y = block1(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 3, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000015?line=1'>2</a>\u001b[0m block2 \u001b[39m=\u001b[39m Residual(\u001b[39m3\u001b[39m,\u001b[39m6\u001b[39m,use_1x1conv\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000015?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m6\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000015?line=3'>4</a>\u001b[0m Y \u001b[39m=\u001b[39m block2(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000015?line=4'>5</a>\u001b[0m Y\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/zane/miniforge3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb Cell 10'\u001b[0m in \u001b[0;36mResidual.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=29'>30</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=31'>32</a>\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zane/Documents/GitHub/LearningPytorch/CNN/ResNet.ipynb#ch0000009?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(Y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# 输出和输入形状不一致的情况\n",
    "block2 = Residual(3,6,use_1x1conv=True,strides=2)\n",
    "X = torch.rand(4,3,6,6)\n",
    "Y = block2(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8bc75a386d56b6551c17e1904f8529f6445721752a10015ef716c860769d0c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
